# LLM sandbox

Just experimenting with local chat LLMs.

To add a model ([available models](https://ollama.com/search)):

```
docker compose exec ollama ollama pull MODEL_NAME
```

